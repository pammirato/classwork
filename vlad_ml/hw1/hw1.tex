\documentclass{article}
\include{hwdefs}
\usepackage{slashbox}
\setcounter{HW}{1}
\begin{document}

\author{Wile E. Coyote}
\title{COMP  790-125, HW\theHW}
\maketitle



\newproblem{0.01pt} Open \texttt{hw\theHW.tex}, replace ``Wile E. Coyote'' with your name. Run
\texttt{pdflatex hw\theHW.tex}, look at hw\theHW.pdf, and confirm that your name is in the right place.


\newproblem{0.5pt}
\begin{enumerate}
\item Plot the sigmoid function in MATLAB using script
\begin{verbatim}
z = [-5:0.1:5];
fz = 1./(1 + exp(-z));
plot(z,fz,'LineWidth',3);
xlabel('z');ylabel('f(z)'); % we always label axes, yes we do!
hwplotprep
print -dpdf sigmoid.pdf
\end{verbatim}
Find the resulting figure in file {\tt sigmoid.pdf}.
\item In hw\theHW.tex, find the segment of the file that sets up the first figure -- it starts with {\tt \textbackslash begin\{figure\}} and ends with  {\tt \textbackslash end\{figure\}}. Inside this segment  replace {\tt emptiness.pdf} with {\tt sigmoid.pdf}.
\item Change the text under {\tt \textbackslash caption} -- right now it says ``This is emptiness, it earns no points.'' -- to say what the figure is about.
\item Remake hw\theHW.pdf by running in shell/command prompt

     \texttt{pdflatex hw\theHW.tex}

and check that your plot and caption are now in.
\end{enumerate}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{emptiness.pdf}
\caption{This is emptiness, it earns no points.}
\end{center}
\end{figure}

\hrule

\newproblem{0.5pt}
Fill in the first derivative and second derivative of sigmoid function in the hw\theHW.tex.

The first derivative
\[
\frac{d f(z)}{dz} =  \answer.%here goes your derivative, but before the % character otherwise it will be commented out
\]

You might have to consult an intro to \LaTeX in order to figure out how to format your math.

\newproblem{0.5pt}
Write a MATLAB function that implements computation  of the first derivative of $f$ at a particular point. You just did the math for this.
Here is a function that is {\em wrong}
\begin{verbatim}
function d = dsigmoid(z)
% This function computes first derivative of sigmoid function at z
d = ...
end
\end{verbatim}
Correct hw\theHW.tex by replacing {\tt ...} above with the correct MATLAB code to compute expression you obtained in previous problem.

Crate a file {\tt dsigmoid.m} that {\em correctly} computes the first derivative.

\newproblem{0.5pt}

We will use your function {\tt dsigmoid.m} to plot the first derivative.
\begin{verbatim}
zs = [-5:0.01:5];
for i = 1:length(zs)
    ds(i) = dsigmoid(zs(i));
end
plot(zs,ds,'LineWidth',3);
xlabel('z');ylabel('df(z)');
hwplotprep
print -dpdf dsigmoid.pdf
\end{verbatim}

Find the resulting plot in file {\tt dsigmoid.pdf}. In hw\theHW.tex replace {\tt emptiness.pdf} with {\tt dsigmoid.pdf} . Change the
caption in the figure to say what the figure is about. Remake hw\theHW.pdf and check that your plot has made it in.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{emptiness.pdf}
\caption{This is the emptiness, it earns no points.}
\end{center}
\end{figure}


\newproblem{0.5pt}
We can approximate derivatives numerically
\[
\frac{df(z)}{dz}\approx \frac{f(z+h) - f(z)}{h}
\]
where the right-side of this approximate equality is called {\em finite difference} approximation. Unlike derivative definition we do not need $h$ to be infinitesimal, just a small value. The numerical approximation of a derivative is tremendously useful trick to check you derivative, gradients, Jacobians, Hessians etc. Make sure that you understand what it does.

We will use this approximation to check your derivatives. Here is a function that computes approximately the derivatives of sigmoid
\begin{verbatim}
function d = fdsigmoid(z)
f0 = 1/(1 + exp(-z));
f1 = 1/(1 + exp(-(z + 1e-5)));
d = (f1 - f0)/1e-5;
end
\end{verbatim}
Save this function into a file names \texttt{fdsigmoid.m}.

Try following code in MATLAB
\begin{verbatim}
zs = randn(100,1);
for i=1:length(zs)
    err(i) = dsigmoid(zs(i)) - fdsigmoid(zs(i));
end
hist(err,30)
hwplotprep
print -dpdf hist.pdf
\end{verbatim}
The code above samples 100 normally distributed values and computes the finite differences approximation and the derivative you derived and implemented and then plots histogram of errors.

Find the resulting plot in file {\tt hist.pdf}. In hw\theHW.tex replace {\tt emptiness.pdf} with {\tt hist.pdf} . Change the
caption in the figure to say what the figure is about. Remake hw\theHW.pdf and check that your plot has made it in.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{emptiness.pdf}
\caption{This is the emptiness, it earns no points.}
\end{center}
\end{figure}



\begin{remark} The error ranges between \answer and \answer.
\end{remark}


\newproblem{0.5pt}
Let
\BEQ\label{eq:pz}
f(z) = \frac{1}{1 + \myexp{-z}} = p
\EEQ
express $z$ in terms of $p$
\[
z = \answer.
\]
Now suppose
\BEQ\label{eq:qz}
\frac{\myexp{-z}}{1 + \myexp{-z}} = q
\EEQ
and express $z$ in terms of $q$
\[
z = \answer.
\]
Given Eqs.\eqref{eq:pz},\eqref{eq:qz} express $q$ in terms of $p$
\[
q = \answer.
\]
Express $f(-z)$ in terms of $f(z)$
\[
f(-z) = \answer.
\]
Hint: the manipulations that are useful here are either subtraction from 1 (as in $1-x$), computing inverse (as in $\frac{1}{x}$), and taking logarithm (as in $\log(x)$).

\section*{Log of sigmoid}
\newproblem{0.5pt}
Let $g(z)$ be log of sigmoid function
\[
g(z) = \mylog{ \frac{1}{1+ \myexp{-z}} }.
\]
Compute its derivative and fill it in here
\[
\frac{dg(z)}{dz} =  \answer.%here goes your derivative of log sigmoid, but before the % character otherwise it will be commented out
\]
Check your derivative by comparing its value to the finite difference approximation.


\newproblem{0.5pt}
Compute second derivative of $g(z)$
\[
\frac{d^2g(z)}{d^2z} =  \answer.%here goes your second derivative of log sigmoid, but before the % character otherwise it will be commented out
\]
Check the second derivative by comparing its value to the finite difference of the {\em first} derivatives you computed above.


\newproblem{0.5pt}
Let the dataset be specified by $\mathcal{D} = \left\{ (\xx_i,y_i):i=1,\dots,n \right\}$. We specify conditional probability of $y$
\BEQ \label{eq:plr}
p(y_i | \xx_i,\beta_0,\beta) = \frac{1}{1 + \myexp{-y_i(\beta_0 + \bket{\beta}{\xx_i})}}
\EEQ
Write a matlab function that computes log probability of label $y$ given a vector of features $\xx$ and $\beta_0,\beta$.
\begin{verbatim}
function logP = logProbLogReg(y,x,beta0,beta)
logP = log( .... )
\end{verbatim}
Now write a matlab function that uses the above function to compute log probability of label $+1$ for a vector of features $\xx$ and $\beta_0,\beta$
\begin{verbatim}
function predY = predictY(x,beta0,beta)
logProbY = logProgLogReg(1,x,beta0,beta);
if logProbY > ...
    predY = ...
else
    predY = ...
end
\end{verbatim}
Hint: Since $p(y=1|x) + p(y=-1|x) = 1$, what is the threshold $p(y=1|x)$ has to exceed for you to predict that $y$ is 1? Consequently what is the threshold
that $\log p(y=1|x)$  has to exceed for you to predict that $y$ is 1?

Generalize this code so that it works for a matrix $\texttt{X}$ with each row being a sample, and returns predicted label for each sample.
\begin{verbatim}
function predY = predictY(X,beta0,beta)
...
\end{verbatim}

\newproblem{0.5pt}
Given Eq.\eqref{eq:plr} we can write out log-likelihood
\BEQ \label{eq:ll}
\textrm{ALL}(\beta_0,\beta;\mathcal{D}) = \frac{1}{N}\sum_{i=1}^N \log \frac{1}{1 + \myexp{-y_i(\beta_0 + \bket{\beta}{\xx_i})}}.
\EEQ
Now using function $\texttt{logProbLogReg}$ that you obtained for the previous problem, write a matlab function that computes loglikelihood
\begin{verbatim}
function val = AverageLogLikLogReg(y,X,beta0,beta)
val = 0;
for i=1:length(y)
   val = val + ...
end
\end{verbatim}
\newproblem{0.5pt}
Write a function that computes gradient of log-likelihood of logistic regression Eq.\eqref{eq:ll}
\begin{verbatim}
function [dbeta0,dbeta] = dAverageLogLikLogReg(y,X,beta0,beta)
dbeta0 = ...
for i=1:length(beta)
    dbeta(i) = ...
end
\end{verbatim}
You can make sure that your implementation is correct using the finite differences trick.
\newproblem{0.5pt}
We will add ridge penalty term to log-likelihood

\BEQ \label{eq:pll}
\textrm{ALL}(\beta_0,\beta;\mathcal{D}) = \left(\frac{1}{N}\sum_{i=1}^N \log \frac{1}{1 + \myexp{-y_i(\beta_0 + \bket{\beta}{\xx_i})}}\right) + \lambda\sum_{j=1}^p \beta_j^2.
\EEQ

Note that ridge penalty does {\em not} apply to $\beta_0$.

Change functions {\texttt AverageLogLikLogReg} and {\texttt dAverageLogLikLogReg} so that they compute penalized average-log-likelihood, and its gradient, respectively.

\begin{verbatim}
function val = AverageLogLikLogReg(y,X,beta0,beta,lambda)
val = 0;
for i=1:length(y)
   val = val + ...
end

function [dbeta0,dbeta] = dAverageLogLikLogReg(y,X,beta0,beta,lambda)
dbeta0 = ...
for i=1:length(beta)
    dbeta(i) = ...
end
\end{verbatim}
You can use finite differences to check the gradient.
\newproblem{2pt}
Implement a gradient ascent algorithm for fitting logistic regression and paste it below.
Remember, gradient ascent iterates updates to parameters by taking a step in the direction of the gradient.  
\begin{verbatim}
function [beta0,beta] = fitLogReg(trainY,trainX,lambda,s)
...
\end{verbatim}
\newproblem{1pt} In Matlab load data stored in \texttt{hw\theHW.mat}. There are six variables stored in this environment:
\texttt{trainX,trainy,validX,validy,testX,testy}. First two variables store a training set, second two a validation set, and the last two a test set.
You will inspect the dataset. 

Visualize sample $i$'s features by running 
\begin{verbatim}
imagesc(reshape(trainX(i,:),[192 168]));colormap(gray)
\end{verbatim}

What is the range of values in each sample's feature vector (\texttt{trainX(i,:)})? \answer

How many different label values are in the set (look at \texttt{trainy})? \answer

What is the nature of the data? \answer

\answer
\newproblem{2pt}
You will fit penalized logistic regression using the code you developed earlier. You will run this code for different combinations of step-sizes ( $10^{-1}$, $10^{-2}$, $10^{-3}$, $10^{-4}$, $10^{-5}$) and for penalty weight \texttt{lambda} ($0.001$, $0.01$, $0.1$, $0.2$, $0.4$, $0.8$, $1$) on the data stored in \texttt{hw\theHW.mat}.

Recall that \texttt{load hw\theHW.mat} loads the \texttt{trainy} and \texttt{trainX} variables.
You can train on this dataset by running command
\begin{verbatim}
[beta0,beta] = fitLogReg(trainy,trainX,lambda,s) 
\end{verbatim}

Write code that runs this fitting procedure for each step-size and $\lambda$ pair and stores resulting \texttt{beta0} and \texttt{beta}.
\begin{verbatim}
stepsizes = 10.^[-1 -2 -3 -4 -5];
lambdas = [0.001 0.01 0.1 0.2 0.4 0.8 1.0];
for i=1:length(stepsizes)
    for j=1:length(lambdas)
     ...
    end
end
\end{verbatim}
This will amount to running the fitting procedure 35 times and getting new \texttt{beta0} and \texttt{beta} each time. 


Using stored \texttt{beta0} and \texttt{beta}, make predictions on validation set \texttt{validX} and compare the predicted labels to \texttt{validy}. 
If a predicted label differs from the corresponding value in \texttt{validy}, that sample has been misclassified. Misclassification error is the fraction of misclassified samples in 
the set. For example, if there are 10 misclassification errors on a set of size 135, then the misclassification error will be $\frac{10}{135} = 0.0741$.

On the validation set, compute misclassification error for each of the 35 stored \texttt{beta0} and \texttt{beta}. 
\begin{verbatim}
for i=1:length(stepsizes)
    for j=1:length(lambdas)
        err(i,j) = ...
    end
end
\end{verbatim}
Populate the table below with these errors. 


\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\backslashbox{\texttt{s}}{\texttt{lambda}}&0.001&0.01&0.1&0.2&0.4&0.8&1 \\\hline
$10^{-1}$ &&&&&&&\\\hline
$10^{-2}$ &&&&&&&\\\hline
$10^{-3}$ &&&&&&&\\\hline
$10^{-4}$ &&&&&&&\\\hline
$10^{-5}$ &&&&&&&\\\hline
\end{tabular}
\end{center}
\newproblem{1pt}
Given the above validation errors, select the \texttt{beta0} and \texttt{beta} that achieve lowest misclassification error on the validation set.

For the selected \texttt{beta0} and \texttt{beta}, evaluate misclassification error on the test set, \texttt{testX} and \texttt{testy}. Report the \texttt{beta0}. 
\begin{verbatim}
beta0 = ...
\end{verbatim}
Sort the entries in \texttt{beta} based on their absolute value and the top 10 entries' indices.
\begin{verbatim}
indices_top10 = ...
\end{verbatim}
Report test set misclassification error.
\begin{verbatim}
testerr = ...
\end{verbatim}
\newproblem{2pt}
$\beta$ vector contains weights of entries in a feature vector. Hence, you can determine which features are more important than others by comparing their weights.
You already identified top 10 features based on their weights.
Explain which features are useful for correctly predicting the label, and why. 
Argue strengths and weaknesses of the dominant features.
\answer
\end{document}
